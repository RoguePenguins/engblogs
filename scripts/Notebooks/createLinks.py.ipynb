{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989f22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "from models import Link,Post, Base\n",
    "from parse_utils import get_full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053b31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SQLAlchemy\n",
    "load_dotenv()\n",
    "\n",
    "engine = create_engine(os.getenv(\"DATABASE_URL\"))\n",
    "\n",
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7a0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link(company, url):\n",
    "    session = Session()\n",
    "    link = Link(company=company, link=url)\n",
    "    session.add(link)\n",
    "    try:\n",
    "        session.commit()\n",
    "        print(f\"Link created for {company} with ID: {link.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving link to database: {e}\")\n",
    "        session.rollback()\n",
    "    finally:\n",
    "        session.close()\n",
    "    return link\n",
    "\n",
    "def get_all_links():\n",
    "    session = Session()\n",
    "    links = session.query(Link).all()\n",
    "    session.close()\n",
    "    return links\n",
    "\n",
    "def get_all_posts():\n",
    "    session = Session()\n",
    "    posts = session.query(Post).all()\n",
    "    session.close()\n",
    "    return posts\n",
    "\n",
    "def delete_link(url):\n",
    "    session = Session()\n",
    "    link = session.query(Link).filter_by(link=url).first()\n",
    "    if link is None:\n",
    "        print(f\"No link found for url {url}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        session.delete(link)\n",
    "        session.commit()\n",
    "        print(f\"Link deleted for url {url}\")\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()  # Roll back the transaction on error\n",
    "        print(f\"Error occurred while deleting link from database: {e}\")\n",
    "    finally:\n",
    "        session.close()  # Always close the session when you're done with it\n",
    "        \n",
    "        \n",
    "def delete_post(title):\n",
    "    session = Session()\n",
    "    try:\n",
    "        post = session.query(Post).filter_by(title=title).first()\n",
    "        session.delete(post)\n",
    "        session.commit()\n",
    "        print(\"All posts deleted.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        session.rollback()  # Roll back the transaction on error\n",
    "        print(f\"Error occurred while deleting posts from database: {e}\")\n",
    "    finally:\n",
    "        session.close()  # Always close the session when you're done with it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5622e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link created for databricks with ID: 11\n"
     ]
    }
   ],
   "source": [
    "# List of URLs and companies\n",
    "data = [\n",
    "    {\"company\": \"databricks\", \"url\": \"https://www.databricks.com/blog/category/industries/financial-services/feed\"}]\n",
    "\n",
    "# Create Link objects and fetch full text for each URL\n",
    "for item in data:\n",
    "    link = create_link(item[\"company\"], item[\"url\"])\n",
    "#     full_text = get_full_text(item[\"url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb222f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.Link at 0x118df8640>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78202107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts: 10\n",
      "links: 3\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year July 7, 2023 in Data Strategy This is part six of a multi-part series to share key insights and tactics with Senior Executives leading data and AI transformation initiatives. You can read part five of the series here. Beginning in 1987, Southwest Airlines famously standardized on flying a single airplane type — the Boeing 737 class of aircraft. This decision allowed the airline to save on both operations and maintenance — requiring only one type of simulator to train pilots, streamlining their spare parts supply chain and maintaining a more manageable parts inventory. Their pilots and maintenance crews were effectively interchangeable in case anyone ever called in sick or missed a connection. The key takeaway is that in order to reduce costs and increase efficiency, Southwest created their own version of a unified platform — getting all their flight-related personas to collaborate and operate from the same point of view. Lessons learned on the platform could be easily shared and reused by other members of the team. The more the team used the unified platform, the more they collaborated and their level of expertise increased. Architectures of enterprise data warehouses (EDWs) and data lakes have proven to be limited and complex — resulting in increased time-to-market and costs. This was mainly due to requirements to perform ETL in order to explore data in the EDW or the need to split data using multiple pipelines for the data lake. The Data Lakehouse architecture simplifies the cost allocation because all the processing, serving and analytics are performed in a single compute layer. Organizations can right-size the data environments and control cost using policies. The centralized and consistent approach to security, auditing and monitoring makes it easier to spot inefficiencies and bottlenecks in the data ecosystem. Performance improvements can be gained quickly as more platform expertise is developed within the workforce. The Databricks Lakehouse platform optimizes cost for your data and AI workloads by intelligently provisioning infrastructure only as you need it. Customers can establish policies that govern the size of clusters based on DEV, TEST, PROD environments or anticipated workloads. As previously mentioned, data transformation initiatives require substantial funding. Centralizing the budget under the CDO provides consistency and visibility into how funds are allocated and spent — increasing the likelihood of a positive ROI. Funding at the beginning of the initiative will be significantly higher than the funding in the out-years. It’s not uncommon to see 3- to 5-year project plans for larger organizations. Funding for years 1 and 2 is often reduced in years 3 and 4 and further reduced in year 5 — until it reaches a steadystate that is more sustainable. The budget takes into account the cost of the data engineering function, commercial software licenses and building out the center of excellence to accelerate the data science capabilities of the organization. Again, the CDO must partner closely with the CIO and the enterprise architect to make sure that the resources are focused on the overall implementation plan and to make sound build vs. buy decisions. It’s common to see the full budget controlled by the CDO, with a significant portion allocated to resources in the CIO’s organization to perform the data engineering tasks. The data science community reports into the CDO and is matrixed into the lines of business in order to better understand the business drivers and the data sets. Finally, investing in data governance cannot wait until the company has suffered from a major regulatory challenge, a data breach or some other serious defense-related problem. CDOs should spend the necessary time to educate leaders throughout the organization on the value of data governance. To establish the centralized budget to fund the data transformation initiative, some organizations impose a “tax” on each part of the organization — based on size as well as profit and loss. This base-level funding should be used to build the data engineering and data science teams needed to deploy the building blocks of the new data ecosystem. However, as different teams, departments and business units begin using the new data ecosystem, the infrastructure costs, both compute and storage, will begin to grow. The costs will not be evenly distributed, due to different levels of usage from the various parts of the organization. The groups with the heavier usage should obviously cover their pro rata share of the costs. This requires the ability to monitor and track usage — not only based on compute but also on the amount of data generated and consumed. This so-called chargeback model is an effective and fair way to cover the cost deltas over and above the base-level funding. Plus, not all the departments or lines of business will require the same level of compute power or fault tolerance. The architecture should support the ability to separate out the runtime portions of the data ecosystem and isolate the workloads based on the specific SLAs for the use cases in each environment. Some workloads cannot fail and their SLAs will require full redundancy, thus increasing the number of nodes in the cluster or even requiring multiple clusters operating in different cloud regions. In contrast, less critical workloads that can fail and be restarted can run on less costly infrastructure. This makes it easier to better manage the ecosystem by avoiding a one-size-fits-all approach and allocating costs to where the performance is needed most. The modern data architecture using Databricks Lakehouse makes it easy to monitor and record usage and allows organizations to easily track costs on a data and AI workload basis. This provides the ability to implement an enterprise-wide chargeback mode and put in place appropriate spending limits. To learn how you can establish a centralized and cohesive data management, data science and data governance platform for your enterprise, please contact us today. This blog post, part of a multi-part series for senior executives, has been adapted from the Databricks’ eBook Transform and Scale Your Organization With Data and AI. Access the full content here. Implementing a successful data strategy requires a thoughtful approach to people and processes. Join us at the Data & AI Summit from June 26-29 to find out how to align goals, identify the right use cases, organize and enable teams, mitigate risk and operate at scale so you can be even more successful with data, analytics and AI. See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year June 23, 2023 in Data Strategy Databricks’ mission is to “democratize access to data analytics and AI.” Not only does that statement give meaning to the everyday work of data professionals, but it is also relevant — reflecting the state of today’s data and AI space because scaling data and AI is hard. Multiple independent surveys and research notes from the likes of McKinsey, Deloitte and Accenture point to the same conclusion: while data and AI demand and interest is at an all-time high, most companies are struggling to achieve enterprise value for data and AI at scale.  One such study is the 2022 Accenture report called “The art of AI maturity”, which showed that only 12% of the 1,200 companies surveyed are realizing a strong competitive advantage and could call themselves data and AI achievers. That’s 88% of them leaving the full value of data and AI untapped. Enterprises need to steer the best path forward in managing the people, process and technology aspects of transformation as a whole to maximize value from data and AI investment. In this blog, we’ll walk you through how Databricks have helped many customers in this journey.   We’ve been working with the world’s top enterprises to help them solve their toughest data and AI problems on a massive scale. Drawing upon these experiences and lessons learned over the last 10 years, we’ve formed our point of view and methodology for how we can optimally help customers build their data and AI practice at scale. Seeing hundreds of customers embark on the lakehouse journey, we saw a pattern exhibited by the ones that are most successful — the true game changers —  in how they manage the following four areas in what we refer to as the four tenets of a successful data and AI business.     In each of the four tenets, Databricks have partnered with customers with the following end goals in mind.   The key organizational construct that is found in these data-native companies is the creation of a center of excellence (CoE) that is designed to establish in-house expertise around ML and AI, and which is then used to educate and scale the rest of the organization on their data and AI practice embodied by the four tenets. It does so by bringing different stakeholders together, providing the right expertise to business units, tracking key projects, helping them move faster, and sharing best practices.  These companies take the stand that building CoE capabilities is not just a one-time exercise. Successful customers treat it as a journey, going through different phases as laid out below in “Establish, Scale and Autonomy.” So the following figure represents the “What” of the Lakehouse CoE framework at a high level and provides a summary view of the key CoE capabilities customers should build and validate along their journey. This represents what a “good” looks like and how customers get there through different phases as they mature. Each red rectangle highlighted above represents CoE milestones. For example, for the “Data & AI Blueprint” tenet, during the “ESTABLISH” phase, customers should build and document robust data models and governance along with the adoption of a well-architected Lakehouse. You need such a blueprint established at this early stage to inform your downstream activities in how you build your data products and applications and in how you run your platform optimally aligned to meet your business objectives. In the “SCALE” phase, then you apply the outcome of the ESTABLISH phase to help business units scale their key business initiatives and day-to-day data activities. For example, with the “Integrate DevOps Practices” milestone for the “Lakehouse Operations” tenet, customers should fully adopt CI/CD in their development practices for developing data products that can be leveraged and reused by other business units. These milestones serve as CoE building blocks with their supporting work breakdown structure and effort required informed by work that has already been done, validated with our experts and mutually agreeing on the right level of help customers need. This approach along with an assessment of the customer's maturity helps Databricks and the customer put together a comprehensive success plan/services roadmap that addresses both short-term needs balanced with long-term data and AI vision. What it really comes down to in measuring success in this endeavor is based on customers developing robust CoE capabilities with self-sufficiency in managing their data and AI practice at scale.    While we’ve been largely talking about the Lakehouse CoE framework and approach, it’s equally important for customers to consider how they should organize their people and process for scale: customers need to build a strong data and AI culture.  To tie together all of the points above, you need to create a Lakehouse Center of Excellence, which will consolidate cross-functional proficiency in digital technologies such as AI and IoT by bringing different stakeholders together, prioritizing and tracking projects, helping them move faster, sharing with the rest of the organization best practices gleaned from business units within and what Databricks is seeing in the industry — along with talent transformation driving upskilling through data and AI education. So if this idea makes sense, in what manner should customers organize and run the CoE? CoE operating models can take on different flavors such as a centralized or distributed approach. Some customers have taken the distributed approach further by leveraging data mesh architecture by organizing data and data products by specific business domains.   A centralized model is shown below, where a central, shared team supports use cases across the organization. Key benefits include the relative ease of developing and governing processes, consistent definitions and use of KPIs, and manageable effort in establishing a single source of truth. While it may not fit everyone, if you are getting started with CoE, this might be a good option to explore further.    So where have we done this? Let’s highlight some of the representative engagements where the Lakehouse CoE partnership with customers has made a meaningful impact.  We’ll cover the first example from the table below. For this multinational investment bank and financial services company, Databricks has partnered with them across four tenets over three years. Toward the middle of the engagement, we observed plateauing of platform usage uptake due to a lack of skills in using the platform. We worked with the customer to help define a comprehensive enablement strategy. In addition to offering customer-tailored training, we defined learning pathways for utilizing self-paced training leading to certification goals integrated as part of their personal development in support of their Certified Engineer and Engineering Excellence initiatives.  Now we have 1,800+ upskilled users and 700+ badges with around 350 in the last 6 months where these users are using the platform to get faster insight into managing their day-to-day activities. In addition, we collaborated in building Data & AI blueprints, focused on use case accelerators to help define reusable components and publish them on an internal portal for consumption across business units. This portal also curates contents and links to the training, recordings from customer user community events and other sources, making it accessible and scalable in a self-service manner. Databricks Professional Services has been partnering with business units as a multi-skilled team to drive optimizations and cost savings in the Lakehouse Operations tenet. These close partnerships have resulted in Databricks being attributed to a $715M three-year value forecast.   These CoE engagements demonstrate how customers across different industries were able to reduce TCO, drive efficiency and scale, and accelerate their business outcomes. At its core, the Lakehouse CoE engagement is made up of 3 components:  These components are used at varying levels of engagement reflecting customer's needs, summarized in the image below. In closing, Lakehouse CoE is a proven delivery framework and methodology that has been hardened by helping many customers solve their toughest data and AI problems at massive scale. Let us know how we can help you accelerate scaling your data and AI practice. We invite readers of this blog whether you are a data engineer, data scientist, analyst, or business/IT leader such as CIO, CDO and CTO to engage with us in discovering how we can partner with you to achieve enterprise value for data and AI at scale. We can be reached at [email protected].  We also encourage you to check out the Databricks Professional Services page to learn more. See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year June 2, 2023 in Data Strategy A key piece of your data and AI transformation strategy will involve the decision around which components of the data ecosystem are built by the in-house engineering team and which are purchased through a vendor relationship. There is increased emphasis within engineering teams on taking a \"builder\" approach. In other words, the engineering teams prefer to develop their own solutions in-house rather than rely on vendor products. This \"roll your own'' approach has some advantages — including being able to establish the overall product vision, prioritize features and directly allocate the resources to build the software. However, it is important to keep in mind which aspects of your development effort give you the most competitive advantage. Spend some time working with the data transformation steering committee and other stakeholders to debate the pros and cons of building out various pieces of the data ecosystem. The primary factor should come down to whether or not a given solution offers true competitive advantage for the organization. Does building this piece of software make it harder for your competitors to compete with you? If the answer is no, then it is better to focus your engineering and data science resources on deriving insights from your data. As many engineering leaders know, building your own software is an exciting challenge. However, it does come with added responsibility — namely, managing the overall project timeline and costs, and being responsible for the design, implementation, testing, documentation, training, and ongoing maintenance and updates. You basically are becoming your own software vendor for every component of the ecosystem that you build yourself. When you consider the cost of a standard-sized team, it is not uncommon to spend several million dollars per year building out individual component parts of the new data system. This doesn't include the cost to operate and maintain the software once it is in production. To offset the anticipated development costs, engineering teams will oftentimes make the argument that they are starting with open source software and extending it to meet the \"unique requirements\" of your organization. It's worth pressure testing this approach and making sure that a) the requirements truly are unique and b) the development offers the competitive advantage that you need. Even software built on top of open source still requires significant investment in integration and testing. The integration work is particularly challenging because of the large number of open source libraries that are required in the data science space. The question becomes, \"Is this really the area that you want your engineering teams focused on?\" Or would it be better to \"outsource\" this component to a third party? Even if you decide the software component provides a competitive advantage and is something worth building in-house, the next question that you should ask is, \"How long will it take?\" There is definitely a time-to-market consideration, and the build vs. buy decision needs to also account for the impact to the business due to the anticipated delivery schedule. Keep in mind that software development projects usually take longer and cost more money than initially planned. The organization should understand the impact to the overall performance and capabilities of the daily ecosystem for any features tied to the in-house development effort. Your business partners likely do not care how the data ecosystem is implemented as long as it works, meets their needs, is performant, is reliable and is delivered on time. Carefully weigh the trade-offs among competitive advantage, cost, features and schedule. Perhaps the single most important feature of a modern data stack is its ability to help make data sets and \"data assets\" consumable to the end users or systems. Data insights, model training and model execution cannot happen in a reliable manner unless the data they depend on can be trusted and is of good quality. In large organizations, revenue opportunities and the ability to reduce risk often depend on merging data sets from multiple lines of business or departments. Focusing your data engineering and data science efforts on curating data and creating robust and reliable pipelines likely provides the best chance at creating true competitive advantage. The amount of work required to properly catalog, schema enforce, quality check, partition, secure and serve up data for analysis should not be underestimated. The value of this work is equally important to the business. The ability to curate data to enable game-changing insights should be the focus of the work led by the CDO and CIO. This has much more to do with the data than it does with the ability to have your engineers innovate on components that don't bring true competitive advantage. To learn how you can establish a centralized and cohesive data management, data science and data governance platform for your enterprise, please contact us today. This blog post, part of a multi-part series for senior executives, has been adapted from the Databricks' eBook Transform and Scale Your Organization With Data and AI. Access the full content here. See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year May 26, 2023 in Data Strategy The era of AI is upon us. Every product builder must ask themselves how to leverage new data and AI capabilities, or their product will not survive - even products as dominant as Google Search are being challenged. Traditional descriptive analytics are table stakes. Modern applications must incorporate real-time insights and AI-driven action to meet user expectations. The cloud has enabled a dizzying array of data stack choices that significantly complicate the design and operations of software applications. Product builders that take the best-of-breed approach very quickly find themselves stitching together and managing multiple incompatible data silos. Developer productivity grinds to a halt, and data maintenance costs spiral out of control. Top software companies - from startups like Abnormal Security to enterprises like Adobe - have built products on the Lakehouse. The Databricks Lakehouse combines the strengths of data lakes and traditional data warehouses. It unifies traditional analytics with modern capabilities of AI and real-time so that products builders do not have to choose between the past and the future. This unified approach accelerates developer productivity, reduces costs, and enables leading-edge innovation. To fuel this Data and AI revolution, Databricks is increasing our investments in companies that build on the Databricks Lakehouse platform. Databricks for Startups and Built on Databricks programs provide financial, technical, and GTM investments to help product builders succeed. Product builders usually start with one data use case in mind, and they build quickly with one database. But the data needs for an application grows continuously. It might start with embedded charts, then evolve to streaming alerts, and soon add generative AI. Before long, the product depends on a mosaic of multiple services - some open source, some native to their IaaS platform, and some proprietary. Each product feature takes longer to develop and manage, and innovation and productivity grind to a halt. When building a new product, builders often make the mistake of kicking the can down the road, assuming they won't develop new data and AI capabilities until far down the roadmap. However, the database creep happens quickly for data-forward products. We often see startups requiring multiple data pipelines at a very early stage. Building on the Databricks Lakehouse allows product builders to avoid this database creep.  The Lakehouse unifies data lake and data warehouse architectures by implementing data management characteristics of a data warehouse directly on a low-cost cloud data lake. As a result, applications built on the Lakehouse can access all data - structured, semi-structured, unstructured - and execute any processing the application needs - data engineering, BI, ML, and real-time streaming. VIZIO adopted the Databricks Lakehouse Platform to unify the diverse needs of their data-as-a-service offering. VIZIO is the leading Smart TV manufacturer in America, harnessing data from TVs to power their platform business and create engaging customer experiences. Before Databricks Lakehouse, they had no single platform to run a data-as-a-service business at this scale. So they got creative by stitching together many data services and a data warehouse. As the data volumes and new features grew, managing this system became prohibitively expensive and time-consuming to manage. Furthermore, the data architecture limited their innovation potential. It would have been a massive undertaking to bolt on a separate real-time streaming and production ML system on top of the data warehouse to support advanced new features. Ultimately, Databricks was the only platform that could handle ETL, monitoring, orchestration, streaming, ML, and Data Governance on a single platform. Not only was Databricks SQL + Delta able to run queries faster on real-world data (3x faster and 60% cheaper than any other data warehouse vendor), but they no longer needed to buy other services just to run the platform and add features in the future. In short, Databricks Lakehouse enables building products with no compromise: Product builders free themselves from the constraints and complexities of specialized data stores and can innovate without limits. The Databricks Lakehouse platform enables a separation between the application and data processing, thereby offering flexible options to product builders. The Lakehouse processes data as a microservice that serves the application through APIs, connectors, and Delta Sharing. When building a product, you can choose to build data processing on your Lakehouse, on your customer's Lakehouse, or share processing between both environments. Databricks Lakehouse empowers you to build the best architecture that meets customers' needs. Hunters chose to build the Hunters SOC Platform on their customers' Lakehouse instances. Hunters SOC Platform is a modern SIEM alternative that transforms their customer's Lakehouse into a security data lake. It ingests and performs the ETL of all security-related data into the customer's Databricks Lakehouse using the customer's cloud storage: the customer retains ownership of all the security data. This involves terabytes of data from dozens of security products. Hunters ETL follows the Databricks' Medallion Architecture model in storing the raw data and normalizing the data into a unified schema. While Hunters provides a rich set of analytical capabilities, customers with advanced cybersecurity analytics teams can augment Hunters' capabilities by leveraging Databricks Data Science and Machine Learning capabilities and the partner technologies in the Databricks ecosystem. With this architecture, Databricks customers can attain an end-to-end, security operations platform on their own Databricks Lakehouse Platform, while keeping the flexibility of owning all the data. Databricks for Startups supports startups through their product journey. Databricks invests free credits that allow startups to explore without worry and technical resources to get them building fast. Once customers and partners have built a product, Built on Databricks program offers joint marketing and sales collaboration opportunities to help them grow on the Databricks Lakehouse platform. Kubit's accelerated journey on the Databricks Lakehouse exemplifies the benefits of these programs. Kubit is a startup that is disrupting the product analytics industry, as the first product analytics tool that leverages modern data-sharing capabilities and is designed for the unlimited volume and scale of data in today's enterprises. Established product analytics companies have built their products on proprietary data stacks, which restrict customers' analytics use cases in data silos. Kubit is built on the Databricks Lakehouse Platform to deliver superior flexibility, scalability, and performance. Kubit's platform utilizes Delta Sharing to enable secure and seamless access to customers' product analytics data and full data model. Kubit is able to process trillions of rows - petabytes - of data to serve their large-scale enterprise customers. Building an application of this scale is a daunting endeavor. By joining the Databricks for Startups program, Kubit received free credits and prompt technical advice to immediately start building. The team developed a prototype in weeks and built an enterprise-ready product in 4 months. With a product built on the Databricks Lakehouse, Kubit also participates in the Built on Databricks program and enjoys Databricks support in reaching joint customers. \"Our final product delivered 10x to 20x performance improvement relative to the MVP. Now we're exploring opportunities with joint customers. Databricks' support has been essential to our success, and we couldn't be happier with the outcome.\" said Alex Li, Kubit's CEO. The rise of cloud platforms such as AWS, Azure, and Google Cloud freed software companies from the need to build infrastructure and engendered a decade of unprecedented software innovation. By giving developers a comprehensive unified data platform on which to build, the Databricks Lakhouse is unleashing the next wave of AI software innovation. Learn more See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year May 17, 2023 in Data Strategy Data is a powerful tool that can be used to improve many aspects of our lives, including our health. With the proliferation of wearable fitness trackers, health apps, and other monitoring devices, it has become easier than ever to collect and analyze data about our health. By tracking and analyzing this data, we can gain valuable insights into our health and wellness, enabling us to make more informed decisions about our lifestyles and habits. Health devices, which enable you to track all your health metrics in one place, make it easy to monitor your progress and make informed decisions about your health. This blog shows how you can use your device and its data to provide even more actionable insights. The example I'll walk through uses Apple Healthkit to perform advanced analytics and machine learning, and build a dashboard with relevant KPIs and metrics. The goal is help track my weekly and monthly performance across these metrics so I can monitor and achieve my health goals. Inspired by the blog post \"You Are What You Measure,\" my intent is to measure my way to good health! You can join in the fun too (Github repo here). With the explosion of data volumes at our fingertips and the myriad of tools to acquire, transform analyze and visualize – it is easy to be overwhelmed. The lakehouse architecture simplifies data use cases by providing all of the necessary capabilities available under one platform. In addition to unifying workflows and data teams, the Databricks Lakehouse Platform – powered by Delta Lake –makes data warehouse-level features (like ACID transactions, governance, and performance) available on data lake scale, flexibility, and cost. To power our dashboard and analytics, we'll be leveraging Apple HealthKit, which in addition to great tracking, provides data sharing capabilities from third-party apps in the iOS ecosystem. But to take it a step further, we'll also be using full extent of the lakehouse! It uses a combination of Apache Spark, Databricks SQL, and MLflow to extract further insights, aggregations, and KPI tracking to keep me honest throughout 2023. We'll walk through how to utilize Delta Live Tables to orchestrate streaming ETL process, use a metadata driven ETL framework for data transformation, and expose a dashboard with relevant KPIs to make data-driven actions! In the subsequent sections of this blogpost, we'll show how to: The first step is to make the data available. There are several options to export Apple Healthkit data, including building your own integration with the accompanying APIs or third-party apps. The approach we'll take is documented on the official HealthKit website by exporting directly from the app. Follow these simple instructions below to export your data: As shown in Figure 1, our data will make several stops along the way to visualization. Once data is available on object storage, we'll process it through the Medallion framework – taking raw XML (bronze), breaking out disparate datasets (silver), and aggregating relevant KPIs on minute, hourly, and daily basis to present to the serving tier (gold). To ensure data is available, log into your target Google Drive account (or wherever your export was uploaded to) and find the filename export.zip. Once located, please ensure file permissions reflect \"anyone with the link,\" and copy the link for later use. Now that our data is available, it's time to set up our notebook for data access, governance, and ingestion into Databricks. The first step is to install and import necessary libraries and setup variables we'll be reusing to automate extractions and transformations in later steps. For data acquisition, we'll be using gDown, a neat little library that makes downloading files from Google Drive simple and efficient. At this point, all you have to do is copy your shared link from Google Drive, a destination folder, and expand .zip archive to the /tmp directory. When exploring the contents of export.zip, there are several interesting datasets available. These data include workout routes (in .gpx format), electrocardiograms (in .csv), and HealthKit records (in .xml). For our purposes we are concentrating on export.xml, which tracks most of our health metrics in Apple HealthKit. Please note, if export.xml contains millions of records (like mine does), you may need to increase the size of the Apache Spark driver for processing. Please refer to GitHub for reference. Before proceeding, we'll do a quick review of the dataframe using Bamboolib, which provides a lowcode/no-code approach to exploring the data and applying transformations, changing datatypes, or performing aggregations with minimal code. This will give us great insights into our data and alert us to any possible data quality concerns. Check out EDA notebook As seen in Figure 3, the export.xml consists of more than 3.8M records across 55 types of data. Through this exploration, we see this is a relatively clean dataset, with minimal null values in the value column, which is important because it stores metrics based on the column type. The type column consists of different available metrics – from sleep tracking to heartbeats per minute.  Now that we understand the data shape and relationships, we can move on to the fun stuff! Upon further inspection, the xml - is not so simple after all. As provided by our Bamboolib analysis, although contained in a single XML, it is actually 55 different metrics that are tracked. Enter Lakehouse! We will apply the ETL Medallion framework on the lakehouse to curate our data lake and process our data for downstream consumption by data science and BI teams – all on cheap object storage! Processing and landing this data into the Delta format allows us to keep raw data and start reaping the benefits. As part of the ELT process, we'll be taking advantage of Delta Live Tables (DLT) to automate and simplify our data processing. DLT provides the advantage of a declarative framework to automate functions, which would be otherwise manually developed by engineering teams, including streaming pipeline tasks such as checkpointing and auto-scaling with enhanced autoscaling, data validation tasks with expectations, and pipeline observability metrics. We'll base our subsequent analysis on the 'type' column, which defines the data in the payload. For example, HeartRate type will have different metrics tracked when compared to ActiveEnergyBurned type. As seen below - our xml file actually contains 55+ metrics (at least in my case – YMMV) tracked by Apple healthKit. Each of these sources represents an unique metric being tracked by Apple HealthKit - regarding different aspects of overall health. For example, it will pull metrics from environmental decibel levels and lat/lon of workouts, to heart rate and calories burned. In addition to unique measurements, they provide different timescales - from per workout, per day, to per second measurements – a rich dataset indeed! We want to ensure we track any data quality concerns throughout the pipeline. From our earlier investigation, it seemed that some values might have been measured incorrectly - which might skew our data (heart rate above 200??). Luckily, DLT makes data quality issues manageable using expectations and pipeline metrics. A DLT expectation is a specific condition you expect of the event (for example IS NOT NULL OR x>5), which DLT will take an action on. These actions could be just for tracking purposes (\"dlt.expect\") or could include dropping the event ('dlt.expect_or_drop') or failing the table/pipeline ('dlt.expect_or_fail'). For more information on DLT and Expectations, please refer to the following page (link). As mentioned above, each type will provide unique insights into your overall health. With over 55 different types and metrics, it can be a daunting task to manage. Especially when new metrics and our data sources pop into the pipeline. For this reason, we'll leverage a metadata-driven framework to simplify and modularize our pipeline. For our next step, we deliver over 10 unique sources to individual silver tables with specific columns and necessary transformations to make the data ML and BI-ready. We use the metadata-driven framework, which simplifies and speeds up development across different data sources, to select specific values from our bronze table and transform into individual silver tables. These transforms will be based on the type column in our bronze table. In this example, we'll be extracting a subset of data sources, but the metadata table is easily extended to include additional metrics/tables as new data sources arise. The metadata table is represented below and contains columns that drive our DLT framework, including source_name, table_name, columns, expectations, and comments. We incorporate our metadata table into our DLT pipeline by leveraging looping capabilities available in the python API. Mixing capabilities between SQL and Python makes DLT an extremely powerful framework for development and transformation. We'll read in the Delta table (but could be anything Spark can read into a dataframe; see example metadata.json in repo) and loop thru to extract metadata variables for our silver tables using the table_iterator function. This simple snippet of code, accompanied by a metadata table, will read in data from bronze, and provide extractions and unique columns to over 10 downstream silver tables. This process is further defined in the \"DLT_bronze2silver\" notebook which contains data ingestion (autoloader) and metadata-driven transformations for silver tables. Below is an example of the DLT DAG created based on the different sources available in Apple HealthKit. And clean datasets! Finally, we combine several interesting datasets – in this case, heart rate and workout information. We then create ancillary metrics (like HeartRate Zones) and perform by-minute and day aggregations to make downstream analytics more performant and appropriate for consumption. This is further defined in the 'DLT_AppleHealth_iterator_Gold' notebook in the repo. With our data available and cleaned up, we are able to build out some dashboards to help us track and visualize our journey. In this case, I built a simple dashboard using capabilities included in Databricks SQL to track KPIs that will help me achieve my goals, including workout time, heart rate variability, workout efforts, overall averages, and short and long-term trends. Of course, if you are more proficient in other data visualization tools (like Power BI or Tableau), Databricks can be fully integrated into your existing workflow. Below is a simple dashboard with relevant metrics, split by 7 day and 30 day averages. I like getting a view across KPIs and time all in a single dashboard. This will help guide my activity program to ensure I continuously improve! With such a rich dataset, you can also start delving into ML and analyze all measures of health. Since I'm not a data scientist, I leveraged the built-in capability of autoML to forecast my weight loss based on some gold and silver tables! AutoML provides an easy and intuitive way to train models, automate hyperparameters tuning, and integrate with MLflow for experiment tracking and Serving of model!  Hopefully this experiment provided a consumable introduction to Databricks and some of the great feature functionality available on the platform. Now it's your turn to leverage the power of data to change behavior in a positive way! Get started with your own experiment. See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year May 3, 2023 in Data Strategy This is part four of a multi-part series to share key insights and tactics with Senior Executives leading data and AI transformation initiatives. You can read part three of the series here. Effective data and AI solutions rely more on the amount of quality data available than on the sophistication or complexity of the report, model or algorithm. Google's paper \"The Unreasonable Effectiveness of Data\" demonstrates this point. The takeaway is that organizations should focus their efforts on making sure data citizens have access to the widest selection of relevant and high-quality data to perform their jobs. This will create new opportunities for revenue growth, cost reduction and risk reduction. Most existing data environments have their data stored primarily in different operational data stores within a given business unit (BU), and this creates several challenges: In order to perform analysis, users are forced to log in to multiple systems to collect their data. This is most often done using single-node data science tools and generates unnecessary copies of data stored on local disk drives, various network shares or user-controlled cloud storage. In some cases, the data is copied to \"user spaces\" within production platform environments. This has the strong potential of degrading the overall performance for true production workloads. To make matters worse, these copies of data are generally much smaller than the full-size data sets that would be needed in order to get the best model performance for your ML and AI workloads. Small data sets reduce the effectiveness of exploration, experimentation, model development and model training — resulting in inaccurate models when deployed into production and used with full-size data sets. As a result, data science teams are spending 80% of their time wrangling data sets and only 20% of their time performing analytic work — work that may need to be redone once they have access to the full-size data sets. This is a serious problem for organizations that want to remain competitive and generate game-changing results. Another factor contributing to reduced productivity is the way in which end users are typically granted access to data. Security policies usually require both coarse-grained and fine-grained data protections. In other words, granting access at a data set level but limiting access to specific rows and columns (fine-grained) within the data set. The most common approach to providing coarse-grained and fine-grained access is to use what's known as role-based access control (RBAC). Individual users log on to system-level accounts or via a single sign-on authentication and access control solution. Users can access data by being added to one or more Lightweight directory access protocol (LDAP) groups. There are different strategies for identifying and creating these groups, but typically, they are done on a system-by-system basis, with a 1:1 mapping for each coarse and fine-grained access control combination. This approach to data access usually produces a proliferation of user groups. It is not unusual to see several thousand discrete security groups for large organizations despite having a much smaller number of defined job functions. This approach creates one of the biggest security challenges in large organizations. When personnel leave the company, it is fairly straightforward to remove them from the various security groups. However, when personnel move around within the organization, their old security group assignments often remain intact and new ones are assigned based on their new job function. This leads to personnel continuing to have access to data that they no longer have a \"need to know.\" Having all your data sets stored in a single, well-managed data lake gives you the ability to use partition strategies to segment your data based on \"need to know.\" Some organizations create a partition based on which business unit owns the data and which one owns the data classification. For example, in a financial services company, credit card customers' data could be stored separately from that of debit card customers, and access to GDPR/CCPA-related fields could be handled using classification labels. The simplest approach to data classification is to use three labels: Taking this into account, an organization could implement a streamlined set of roles for RBAC that uses the convention <domain><entity><data set | data asset><classification> where \"domain\" might be the business unit within an organization, \"entity\" is the noun that the role is valid for, \"data set\" or \"data asset\" is the ID, and \"classification\" is one of the three values (public, internal, restricted). There is a \"deny all default\" policy that does not allow access to any data unless there is a corresponding role assignment. Wildcards can be used to grant access to eliminate the need to enumerate every combination. For example, <credit-card><customers><transactions> <restricted> gives a user or a system access to all the data fields that describe a credit card transaction for a customer, including the 16-digit credit card number. Whereas <credit-card><customers><transactions><internal> would allow the user or system access only to nonsensitive data regarding the transaction. This gives organizations the chance to rationalize their security groups by using a domain naming convention to provide coarse-grained and fine-grained access without the need for creating tons of LDAP groups. It also dramatically eases the administration of granting access to data for a given user. The modern data stack, when combined with a simplified security group approach and a robust data governance methodology, gives organizations an opportunity to rethink how data is accessed — and greatly improves time to market for their analytic use cases. All analytic workloads can now operate from a single, shared view of your data. Combining this with a sensitive data tokenization strategy can make it straightforward to empower data scientists to do their job and shift the 80/20 ratio in their favor. It's now easier to work with full-size data sets that both obfuscate NPI/PII information and preserve analytic value. Now, data discovery is easier because data sets have been registered in the catalog with full descriptions and business metadata — with some organizations going as far as showing realistic sample data for a particular data set. If a user does not have access to the underlying data files, having data in one physical location eases the burden of granting access, and then it's easier to deploy access-control policies and collect/analyze audit logs to monitor data usage and to look for bad actors. The modern data architecture using Databricks Lakehouse makes it easy to take a consistent approach to protecting, validating and improving your organization's data. Data governance policies can be enforced during curation using built-in features such as schema validation, data quality \"expectations\" and pipelines. Databricks enables moving data through well-defined states: Raw —> Refined —> Curated or, as we refer to it at Databricks, Bronze —> Silver —> Gold. The raw data is known as \"Bronze-level\" data and serves as the landing zone for all your important analytic data. Bronze data functions as the starting point for a series of curation steps that filter, clean and augment the data for use by downstream systems. The first major refinement results in data being stored in \"Silver-level\" tables within the data lake. As these tables are recommended to use an open table format (i.e. Delta Lake) for storage, they provide additional benefits such as ACID transactions and time travel. The final step in the process is to produce business-level aggregates, or \"Gold-level\" tables, that combine data sets from across the organization. It's a set of data used to improve customer service across the full line of products or look for opportunities to cross-sell to increase customer retention. For the first time, organizations can truly optimize data curation and ETL — eliminating unnecessary copies of data and the duplication of effort that often happens in ETL jobs with legacy data ecosystems. This \"solve once, access many times\" approach speeds time to market, improves the user experience and helps retain talent. Data sharing is crucial to drive business value in today's digital economy. More and more organizations are now looking to securely share trusted data with their partners/suppliers, internal lines of business or customers to drive collaboration, improve internal efficiency and generate new revenue streams with data monetization. Additionally, organizations are interested in leveraging external data to drive new product innovations and services. Business executives must establish and promote a data sharing culture in their organizations to build competitive advantage. Data democratization is a key step on the data and AI transformation journey to enable data citizens across the enterprise irrespective of their technical acumen. At the same time, organizations must have a strong stance on data governance to earn and maintain customer trust, ensure sound data and privacy practices, and protect their data assets. Databricks Lakehouse platform provides a unified governance solution for all your data and AI assets, built-in data quality to streamline data curation and a rich collaborative environment for data teams to discover new insights. To learn more, please contact us. Want to learn more? Check out our eBook Transform and Scale Your Organization With Data and AI. The C-suite dialogue on generative AI\n",
      "Industry experts reveal winning strategies that minimize risk. See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year April 26, 2023 in Data Strategy Data and AI have emerged as strategic imperatives for most organizations over the last few years. Companies big and small have rallied to hire scores of data and AI experts and have made considerable investments in upgrading and evolving their data platforms. Thus far, most of the investments have focused on the technology and the technical skills needed to leverage data and AI, and to solve the technological challenges that have emerged with the scale and complexity of an ever evolving data landscape. At Databricks, we are known for facing those challenges head-on and solving them, be it by inventing the lakehouse paradigm, enabling easier sharing and collaboration with Delta Sharing or by providing end-to-end MLOps capabilities with MLFlow. However, technology alone cannot solve every issue and even the best technology if not used properly is sure to fall short of expectations. This is a trend that we have seen lately in AI. At Databricks we have recognized that having the right operating model, processes and enabling the best ways of working are as essential to the success of companies seeking to make the best use of data and AI as having the right technology, algorithms and skills to develop it. Part I of this series focuses on a general overview of how the Databricks platform supports a modern operating model for AI, while subsequent parts will delve into the details of what it means for teams and organizations. With that in mind, let's dive into how Databricks supports your operating model for AI: Organizations operate in a wide variety of contexts and come in many different shapes and sizes, hence it makes sense that there is no universal operating model for AI that fits everywhere. Regulatory requirements, data and resource availability and many other factors will play a role in determining the right operating model. It is important then, to have a platform that is able to scale with the organization as more use cases and users enter the pipeline, as more models are deployed and as more of the business is enhanced by data and AI. The services that underpin the operating model need to be flexible while at the same time ensuring that security and explainability permeate every step of the process. Databricks helps companies overcome not only their technical challenges by providing state-of-the-art data processing and AI capabilities but it provides the structure and platform on top of which organizations can: Visit Databricks.com to learn more about how Databricks helps its clients solve their toughest challenges with Data and AI! See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year April 19, 2023 in Data Strategy This is part three of a multi-part series to share key insights and tactics with Senior Executives leading data and AI transformation initiatives. You can read part two of the series here. In order to succeed with data, analytics and AI, companies must find and organize the right talent into high performing teams — ones that can execute against a well-defined strategy with the proper tools, processes, training and leadership. Digital transformations require executive-level support and are likely to fail without it — especially in large organizations. However, it’s not enough to simply hire the best data and AI talent — the organization must want to succeed, at an enterprise level. In other words, they must also evolve their company culture into one that embraces data, data literacy, collaboration, experimentation and agile principles. We define these companies as “data native.” Chief Information Officers and Chief Data Officers — two sides of the data coin Data native companies generally have a single, accountable executive who is responsible for areas such as data science, business analytics, data strategy, data governance and data management. The data management aspects include registering data sets in a data catalog, tracing data lineage as data sets flow through the environment, performing data quality checks and scanning for sensitive data in the clear. Many organizations are rapidly adding the Chief Data Officer (CDO) role to their executive ranks in order to oversee and manage these responsibilities. The CDO works closely with CIOs and other business stakeholders to establish the overall project plan, design and implementation — and to align project management, product management, business analysis, data engineering, data scientist and machine learning talent. The CDO and CIO will need to build a broad coalition of support from stakeholders who are incentivized to make the transformation a success and help drive organization-wide adoption. To do this, the stakeholders must understand the benefits of — and their role and responsibilities in — supporting the initiative. There are two organizational constructs that are found in most successful data native companies. The first is the creation of an AI/ML center of excellence (COE) that is designed to establish in-house expertise around ML and AI, and which is then used to educate the rest of the organization on best practices. The second is the formation of a data and AI transformation steering committee that will oversee and guide decisions and priorities for the transformative data, analytics and AI initiatives, plus help remove obstacles. Creating an AI/ML COE Data science is a fast-evolving discipline with an ever-growing set of frameworks and algorithms to enable everything from statistical analysis to supervised learning to deep learning using neural networks. While it is difficult to establish specific and exact boundaries between the various disciplines, for the purposes of this document, we use “data science” as an umbrella term to cover machine learning and artificial intelligence. Organizations wanting to build a data science competency should consider hiring talent into a centralized organization, or COE, for the purposes of establishing the tools, techniques and processes for performing data science. The COE works with the rest of the organization to educate and promote the appropriate use of data science for various use cases. A common approach is to have the COE report into the CDO, but still have data scientists dotted line into the business units or department. Using this approach, you achieve two goals: Data and AI transformation steering committee The purpose of the steering committee is to provide governance and guidance to the data transformation initiative. The CDO and CIO should co-chair the committee along with one business executive who can be a vocal advocate and help drive adoption. The level of executive engagement is critical to the success of the initiative. The steering committee should meet regularly with leaders from across the organization to hear status reports and resolve any conflicts and remove obstacles, if possible. The leaders should represent a broad group of stakeholders, including but not limited to: Partnering with architecture and InfoSec Early on, the CDO and CIO should engage the engineering and architecture community within the organization to ensure that everyone understands the technical implications of the overall strategy. This minimizes the chances that the engineering teams will build separate and competing data platforms. In many cases, a named enterprise architect (EA) or similar will be required and responsible for validating that the overall technology design and data management features support the performance and regulatory compliance requirements — specifically, whether the proposed design can meet the anticipated SLAs of the most demanding use cases and support the volume, velocity, variety and veracity (four Vs) of the data environment. From an InfoSec perspective, the CDO must work to ensure that the proper controls and security are applied to the new data ecosystem and that the authentication, authorization and access control methods meet all the data governance requirements. An industry best practice is to enable self-service registration of data sets, by the data owner, and support the assignment of security groups or roles to help automate the access control process. This allows data sets to be accessible only to the personnel that belong to a given group. The group membership could be based primarily on job function or role within the organization. This approach provides fast onboarding of new employees, but caution should be taken not to proliferate too many access control groups — in other words, do not get too fine grained with group permissions, as they will become increasingly difficult to manage. A better strategy is to be more coarse-grained and use row- and column-level security sparingly. Centralized vs. federated labor strategy In most organizations today, managers work in silos, making decisions with the best intentions but focused on their own functional areas. The primary risk to the status quo is that there will be multiple competing and conflicting approaches to creating enterprise data and AI platforms. This duplication of effort will waste time and money and potentially erode the confidence and motivation of the various teams. While it certainly is beneficial to compare and contrast different approaches to implementing an architecture, the approaches should be strictly managed, with everyone designing for the same goals and requirements — as described in this post and adhering to the architectural principles and best practices. Even still, the roles of the CDO and CIO together should deliver a data analytics and AI platform with the least amount of complexity as possible, and one that can easily scale across the organization. Having the data engineering teams centralized, reporting into a CIO, makes it easier to design a modern data stack — while ensuring that there is no duplication of effort when implementing the platform components. Below shows one possible structure.  To learn how you can establish a centralized and cohesive data management, data science and data governance platform for your enterprise, please contact us today. This blog post, part of a multi-part series for senior executives, has been adapted from the Databricks’ eBook Transform and Scale Your Organization With Data and AI. Access the full content here. The C-suite dialogue on generative AI\n",
      "Industry experts reveal winning strategies that minimize risk. See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year March 28, 2023 in Data Strategy Now more than ever, organizations need to adapt quickly to market opportunities and emerging risks so that they are better positioned to adapt, innovate and thrive in the modern and dynamic economy. Business leaders see digital transformation as an opportunity to build a new technology foundation from which to run their business, while lowering costs and increasing business value. However, conflicting organizational priorities, legacy-based information systems, and disparate data environments make that difficult to achieve. To that end, data, analytics and AI executives need to develop and execute a comprehensive strategy that enables them to easily deploy and transition to a modern data architecture. This blog series will share key insights and tactics that you should consider as you embark on your own journey. To begin, we propose six tactics that should serve as the foundation of every data and platform modernization initiative. To facilitate a multi-cloud strategy, you can follow the same process to migrate off cloud native big data systems such as EMR and Dataproc. It is important, however, to make sure that the organization has the fortitude to hold the line when there is pressure to make changes to the legacy environments or extend their lifespan. Setting firm dates for when these legacy systems will be retired will serve as a forcing function for teams when they onboard to the new modern data architecture. Having the executive buy-in plays a crucial role in seeing the shutdown of legacy platforms through. Conclusion\n",
      "Whether you are just getting started or already in the process of a multi-year strategy, this template can be applied at any time and repeated as you tackle your organization's vast portfolio one project at a time. Going \"all-in\" on the multi-cloud, consolidating all data into the data lake and securing executive sponsorship will provide a solid foundation for your data modernization strategy through pivots in an unpredictable and dynamic business environment. In fact, thousands of customers have already revolutionized their data and AI capabilities by adopting the latest innovation in big (and small) data: the lakehouse–unifying their data warehousing, data lake architecture and use cases on Databricks. Managing distinct and redundant data environments, its respective security and governance controls and paying for each of them are the stories of yesterday for our customers. To learn more, please contact us. Want to learn more? Check out our eBook Transform and Scale Your Organization With Data and AI. The C-suite dialogue on generative AI\n",
      "Industry experts reveal winning strategies that minimize risk. See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n",
      "Discover how to build and manage all your data, analytics and AI use cases with the Databricks Lakehouse Platform Report\n",
      "\n",
      "Tap the potential of AI\n",
      "Explore recent findings from 600 CIOs across 14 industries in this MIT Technology Review report Missed Data + AI Summit?   Data + AI Summit is over, but you can still watch the keynotes and 250+ sessions from the event on demand. Connect with validated partner solutions in just a few clicks. See why Gartner named Databricks a Leader for the second consecutive year February 21, 2023 in Data Strategy Among all the rapid changes brought about by the pandemic, perhaps the most significant has been the emergence of data as a critical public good. However, data's unique characteristics mean that its value erodes when it is seen as untrustworthy. We've all seen the most successful companies and large enterprises fined large sums and suffer massive reputational damage as a result of failing to uphold the highest standards for data management and security. It has therefore become a strategic imperative for companies to ensure their data governance remains at the highest standards, yet flexible. At a recent Economist Impact webinar with National Australia Bank, FWD Insurance, AEON and Databricks, we affirmed how robust data governance can only be derived from a deep understanding of privacy, security, and operational risks, and also driven in sync with ongoing changes across the compliance landscape. We also recognise the myriad of challenges standing in the way of good governance, from a misalignment between data management activities and operating models; a disconnect between legislation and security policies; and a lack of understanding as to how democratized data access generates business transformation. The participants also discussed how data governance has become more complex for organizations operating across borders. The increasing complexities of cross-border data regulation is a pain point that requires more granular management, where data stores and business applications now have different types of data models, complexities, and security apparatus of their own in each market, along with the mandatory compliance and regulatory requirements. Moreover, the need for the right engineering, analytics and data skills was spotlighted as essential to successful data transformation and governance. It was agreed amongst the speakers that not only do businesses need to hire the relevant talent, but existing employees should also be exposed to, upskilled, and educated on new, deeper knowledge, including how the flexibility of open-source tools can be used to unify governance across various disparate IT systems and to truly understand how data moves throughout their organizations. Best practices to maximize data's economic value Weighing out the considerations from all these factors will be no easy task. Organizations must recognise that strong data governance can only be built when they approach it as a journey rather than a destination. That's where using the latest technologies to extract the most value from data can be extremely beneficial. Artificial Intelligence (AI) and Machine Learning (ML) are solutions that can incorporate privacy and security into data management models, process large amounts of data, and handle data on a large scale. New, emerging architectures, such as the Data Lakehouse, which combines cloud-native flexibility with AI and ML, have the potential to assess various and new types of risks as the macroeconomy progresses and matures, analyze how these market risks impact compliance and regulations, and deliver the necessary adjustments to enterprise data. Moreover, companies should aim to groom and develop as many data stewards as possible, and this can be achieved through two-way collaboration in the form of public-private partnerships. Such a goal will allow good governance frameworks and practices to be distributed and decentralized in every department, yet still tightly integrated across the organization. These efforts also need to be supplemented by embedding privacy by design across all platforms, operations and processes of the business, as increasing volumes of data get moved, stored, and managed by internal and external stakeholders. We recognise that companies are under pressure today to continuously rethink their infrastructure but at the same time also need to manage emerging risks that come with rolling out transformation strategies. As a result, key initiatives like moving to the cloud and decommissioning legacy platforms are often approached with a sense of caution and dread. However, data management and governance must be perceived as key components of business growth, and open-source platforms exist as essential tools that can enable the right balance of governance versus accessibility and use. Watch 60 minutes full content on \"Making every byte count: maximizing value by modernizing data governance\" See Careers\n",
      "at Databricks Databricks Inc.\n",
      "160 Spear Street, 13th Floor\n",
      "San Francisco, CA 94105\n",
      "1-866-330-0121 © Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation.\n"
     ]
    }
   ],
   "source": [
    "posts = get_all_posts()\n",
    "links = get_all_links()\n",
    "\n",
    "print(f'posts: {len(posts)}')\n",
    "print(f'links: {len(links)}')\n",
    "\n",
    "# # posts = [post for post in posts]\n",
    "\n",
    "# # # Print first 10...\n",
    "for post in posts:\n",
    "    print(post.fulltext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a393afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All posts deleted.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ddfca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
